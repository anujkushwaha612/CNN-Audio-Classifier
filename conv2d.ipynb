{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "332255df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# A simple 5x5 single-channel image\n",
    "input_array = np.array([\n",
    "    [0, 0, 0, 10, 10],\n",
    "    [0, 0, 0, 10, 10],\n",
    "    [0, 0, 0, 10, 10],\n",
    "    [0, 0, 0, 10, 10],\n",
    "    [0, 0, 0, 10, 10]\n",
    "])\n",
    "\n",
    "# A 3x3 kernel for vertical edge detection\n",
    "kernel = np.array([\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b65a6f",
   "metadata": {},
   "source": [
    "To keep the output size the same as the input when using a 3x3 kernel, our first step is to add a 1-pixel border of padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f08d374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Array:\n",
      " [[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 10 10  0]\n",
      " [ 0  0  0  0 10 10  0]\n",
      " [ 0  0  0  0 10 10  0]\n",
      " [ 0  0  0  0 10 10  0]\n",
      " [ 0  0  0  0 10 10  0]\n",
      " [ 0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "padded_array = np.pad(input_array, pad_width=1, mode='constant', constant_values=0)\n",
    "\n",
    "print(\"Padded Array:\\n\", padded_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a941b1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Patch:\n",
      " [[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "\n",
      "Kernel:\n",
      " [[ 1  0 -1]\n",
      " [ 1  0 -1]\n",
      " [ 1  0 -1]]\n",
      "\n",
      "First Output Pixel Value: 0\n"
     ]
    }
   ],
   "source": [
    "# Get the top-left 3x3 patch\n",
    "patch = padded_array[0:3, 0:3]\n",
    "\n",
    "# Perform the operation\n",
    "output_pixel = np.sum(patch * kernel)\n",
    "\n",
    "print(\"First Patch:\\n\", patch)\n",
    "print(\"\\nKernel:\\n\", kernel)\n",
    "print(\"\\nFirst Output Pixel Value:\", output_pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b9d9ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Array:\n",
      " [[  0.   0. -20. -20.  20.]\n",
      " [  0.   0. -30. -30.  30.]\n",
      " [  0.   0. -30. -30.  30.]\n",
      " [  0.   0. -30. -30.  30.]\n",
      " [  0.   0. -20. -20.  20.]]\n"
     ]
    }
   ],
   "source": [
    "output_array = np.zeros_like(input_array, dtype=np.float32)\n",
    "\n",
    "for y in range(input_array.shape[1]):\n",
    "    for x in range(input_array.shape[0]):\n",
    "        current_patch = padded_array[y : y + 3 , x : x + 3]\n",
    "        \n",
    "        output_array[y,x] = np.sum(current_patch * kernel)\n",
    "        \n",
    "print(\"Output Array:\\n\", output_array)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564c00d6",
   "metadata": {},
   "source": [
    "# Making our custom Conv2d class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fed1f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        kernel_shape = (out_channels, in_channels, kernel_size, kernel_size)\n",
    "        self.kernel = np.random.randn(*kernel_shape) * 0.01\n",
    "        self.bias = np.zeros(out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a0b7537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We manually found 4 parameter arrays.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We'll use our PlainConv2d class from before\n",
    "class PlainConv2d:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        kernel_shape = (out_channels, in_channels, kernel_size, kernel_size)\n",
    "        self.kernel = np.random.randn(*kernel_shape) * 0.01\n",
    "        self.bias = np.zeros(out_channels)\n",
    "    # (It also has a forward method we wrote)\n",
    "\n",
    "# Now, our network class\n",
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        self.conv1 = PlainConv2d(1, 8, 3)\n",
    "        self.conv2 = PlainConv2d(8, 16, 3)\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        # We loop through all attributes of this class\n",
    "        for attr in vars(self).values():\n",
    "            # If an attribute is a layer, get its parameters\n",
    "            if isinstance(attr, PlainConv2d):\n",
    "                params.append(attr.kernel)\n",
    "                params.append(attr.bias)\n",
    "        return params\n",
    "\n",
    "# Let's test it\n",
    "model = SimpleNet()\n",
    "param_list = model.parameters()\n",
    "print(f\"We manually found {len(param_list)} parameter arrays.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e78a7b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recursive method found 8 parameter arrays.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PlainConv2d:\n",
    "    \"\"\"The basic layer with its own parameters.\"\"\"\n",
    "    def __init__(self, in_c, out_c, k_size):\n",
    "        kernel_shape = (out_c, in_c, k_size, k_size)\n",
    "        self.kernel = np.random.randn(*kernel_shape) * 0.01\n",
    "        self.bias = np.zeros(out_c)\n",
    "\n",
    "    def parameters(self):\n",
    "        # A layer just returns its own parameters in a list.\n",
    "        return [self.kernel, self.bias]\n",
    "\n",
    "class ResidualBlock:\n",
    "    \"\"\"A module that contains other modules (layers).\"\"\"\n",
    "    def __init__(self):\n",
    "        self.conv1 = PlainConv2d(8, 8, 3)\n",
    "        self.conv2 = PlainConv2d(8, 8, 3)\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        # This is the key: it calls .parameters() on its children.\n",
    "        params.extend(self.conv1.parameters())\n",
    "        params.extend(self.conv2.parameters())\n",
    "        return params\n",
    "\n",
    "class DeeperNet:\n",
    "    \"\"\"The top-level model, which contains other modules.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.block1 = ResidualBlock()\n",
    "        self.block2 = ResidualBlock()\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        # It also calls .parameters() on its children.\n",
    "        params.extend(self.block1.parameters())\n",
    "        params.extend(self.block2.parameters())\n",
    "        return params\n",
    "\n",
    "# --- Let's test it ---\n",
    "deep_model = DeeperNet()\n",
    "final_params = deep_model.parameters()\n",
    "print(f\"The recursive method found {len(final_params)} parameter arrays.\")\n",
    "# Output should be 8: (2 params/conv) * (2 convs/block) * (2 blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a054828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recursive method found 40 parameter arrays in the 10-block network.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- We'll use the same building blocks from before ---\n",
    "\n",
    "class PlainConv2d:\n",
    "    \"\"\"The basic layer with its own parameters.\"\"\"\n",
    "    def __init__(self, in_c, out_c, k_size):\n",
    "        kernel_shape = (out_c, in_c, k_size, k_size)\n",
    "        self.kernel = np.random.randn(*kernel_shape) * 0.01\n",
    "        self.bias = np.zeros(out_c)\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.kernel, self.bias]\n",
    "\n",
    "class ResidualBlock:\n",
    "    \"\"\"A module that contains other modules (layers).\"\"\"\n",
    "    def __init__(self):\n",
    "        self.conv1 = PlainConv2d(8, 8, 3)\n",
    "        self.conv2 = PlainConv2d(8, 8, 3)\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        params.extend(self.conv1.parameters())\n",
    "        params.extend(self.conv2.parameters())\n",
    "        return params\n",
    "\n",
    "# --- Here is our new, deeper network ---\n",
    "\n",
    "class VeryDeepNet:\n",
    "    \"\"\"A deeper model containing a list of sub-modules.\"\"\"\n",
    "    def __init__(self):\n",
    "        # We create a list of 10 residual blocks\n",
    "        self.blocks = [ResidualBlock() for _ in range(10)]\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        # We loop through our list of blocks\n",
    "        for block in self.blocks:\n",
    "            # And recursively get the parameters from each one\n",
    "            params.extend(block.parameters())\n",
    "        return params\n",
    "\n",
    "# --- Let's test it ---\n",
    "deep_model = VeryDeepNet()\n",
    "final_params = deep_model.parameters()\n",
    "print(f\"The recursive method found {len(final_params)} parameter arrays in the 10-block network.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74b10c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "class NumpyConv2d:\n",
    "    \"\"\"A Conv2d layer built from scratch using only NumPy.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        # Store hyperparameters\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Initialize learnable parameters (weights and bias)\n",
    "        kernel_shape = (out_channels, in_channels, kernel_size, kernel_size)\n",
    "        self.kernel = np.random.randn(*kernel_shape) * 0.01\n",
    "        self.bias = np.random.randn(out_channels) * 0.01\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Performs the forward pass of the convolution.\"\"\"\n",
    "        # Get input dimensions\n",
    "        batch_size, in_channels, in_h, in_w = x.shape\n",
    "\n",
    "        # Pad the input\n",
    "        padded_x = np.pad(x, ((0,0), (0,0), (self.padding, self.padding), (self.padding, self.padding)))\n",
    "\n",
    "        # Calculate output dimensions\n",
    "        out_h = (in_h + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_w = (in_w + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "\n",
    "        # Create empty output tensor\n",
    "        output = np.zeros((batch_size, self.out_channels, out_h, out_w))\n",
    "\n",
    "        # --- The Main Convolution Loop ---\n",
    "        for i in range(batch_size):           # Loop over each image in the batch\n",
    "            for c_out in range(self.out_channels): # Loop over each output channel (filter)\n",
    "                for y in range(out_h):         # Loop over the vertical dimension\n",
    "                    for x in range(out_w):     # Loop over the horizontal dimension\n",
    "                        # Find the corners of the current slice\n",
    "                        y_start = y * self.stride\n",
    "                        y_end = y_start + self.kernel_size\n",
    "                        x_start = x * self.stride\n",
    "                        x_end = x_start + self.kernel_size\n",
    "\n",
    "                        # Get the 3D patch from the padded input\n",
    "                        patch = padded_x[i, :, y_start:y_end, x_start:x_end]\n",
    "                        \n",
    "                        # Get the 3D kernel for this output channel\n",
    "                        kernel_slice = self.kernel[c_out, :, :, :]\n",
    "\n",
    "                        # Perform the element-wise multiplication and sum, then add bias\n",
    "                        output[i, c_out, y, x] = np.sum(patch * kernel_slice) + self.bias[c_out]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ec36f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failure! The outputs do not match.\n",
      "\n",
      "--- Speed Comparison ---\n",
      "Our NumPy Conv: 0.0670 seconds\n",
      "PyTorch nn.Conv2d: 0.001159 seconds\n",
      "\n",
      "PyTorch is roughly 57x faster!\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "batch_size, in_c, h, w = 11, 7, 30, 30\n",
    "out_c, k, s, p = 17, 7, 3, 2\n",
    "\n",
    "# Create random input data\n",
    "numpy_input = np.random.randn(batch_size, in_c, h, w)\n",
    "torch_input = torch.from_numpy(numpy_input).float()\n",
    "\n",
    "# 1. Our NumPy Layer\n",
    "numpy_layer = NumpyConv2d(in_c, out_c, k, s, p)\n",
    "numpy_output = numpy_layer.forward(numpy_input)\n",
    "\n",
    "# 2. The Real PyTorch Layer\n",
    "torch_layer = nn.Conv2d(in_c, out_c, k, s, p)\n",
    "# IMPORTANT: We copy our NumPy weights into the PyTorch layer\n",
    "torch_layer.weight.data = torch.from_numpy(numpy_layer.kernel).float()\n",
    "torch_layer.bias.data = torch.from_numpy(numpy_layer.bias).float()\n",
    "torch_output = torch_layer(torch_input)\n",
    "\n",
    "# --- Verification ---\n",
    "# Convert torch output to numpy to compare\n",
    "torch_output_np = torch_output.detach().numpy()\n",
    "# Check if the outputs are almost identical (to handle tiny floating point differences)\n",
    "if np.allclose(numpy_output, torch_output_np):\n",
    "    print(\"✅ Success! The outputs are identical.\")\n",
    "else:\n",
    "    print(\"❌ Failure! The outputs do not match.\")\n",
    "\n",
    "# --- Performance Test ---\n",
    "print(\"\\n--- Speed Comparison ---\")\n",
    "# Time our NumPy implementation\n",
    "start_time = time.time()\n",
    "numpy_layer.forward(numpy_input)\n",
    "numpy_time = time.time() - start_time\n",
    "print(f\"Our NumPy Conv: {numpy_time:.4f} seconds\")\n",
    "\n",
    "# Time the PyTorch implementation\n",
    "start_time = time.time()\n",
    "torch_layer(torch_input)\n",
    "torch_time = time.time() - start_time\n",
    "print(f\"PyTorch nn.Conv2d: {torch_time:.6f} seconds\")\n",
    "\n",
    "# Calculate how much faster PyTorch is\n",
    "if torch_time > 0:\n",
    "    print(f\"\\nPyTorch is roughly {int(numpy_time / torch_time)}x faster!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0620b63e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
